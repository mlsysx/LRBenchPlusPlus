{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a412b4f-515e-4a10-9eb4-ad81195b7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,3,2,1,5,6,7,0\"\n",
    "import tqdm \n",
    "from datetime import datetime  \n",
    "import subprocess \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "script_location = '../train/train.py'\n",
    "model_location = ''\n",
    "log_location = ''\n",
    "\n",
    "\n",
    "\n",
    "learning_rates = [3e-5] # 1e-05, 2e-5, 6e-5\n",
    "for lr in learning_rates:\n",
    "    current_time = datetime.now()\n",
    "    print(lr, current_time, \"---\" * 20)\n",
    "    \n",
    "    command = f\"\"\"torchrun --nproc_per_node=8 --master_port=6009 {script_location} \\\n",
    "        --data_path ./train/alpaca_data.json \\\n",
    "        --bf16 True \\\n",
    "        --num_train_epochs 3 \\\n",
    "        --per_device_train_batch_size 4 \\\n",
    "        --per_device_eval_batch_size 16 \\\n",
    "        --gradient_accumulation_steps 4 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"epoch\" \\\n",
    "        --save_total_limit 10 \\\n",
    "        --learning_rate {lr} \\\n",
    "        --weight_decay 0. \\\n",
    "        --warmup_ratio 0. \\\n",
    "        --logging_steps 1 \\\n",
    "        --deepspeed \"./train/configs/default_offload_opt_param-WarmupLR.json\" \\\n",
    "        --output_dir  {model_location}/output_WarmupLR_{lr} \\\n",
    "        --tf32 True > {log_location}/output_WarmupLR_{lr}.txt\"\"\"\n",
    "    \n",
    "    result = subprocess.run(command, shell=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rates = [1e-5, 6e-5] # 2e-5, 3e-5, 4e-5, 5e-5, 6e-5 \n",
    "for lr in learning_rates:\n",
    "    current_time = datetime.now()\n",
    "    print(lr, current_time, \"---\" * 20)\n",
    "    \n",
    "    command = f\"\"\"torchrun --nproc_per_node=8 --master_port=6009 {script_location} \\\n",
    "        --data_path ./train/alpaca_data.json \\\n",
    "        --bf16 True \\\n",
    "        --num_train_epochs 3 \\\n",
    "        --per_device_train_batch_size 4 \\\n",
    "        --per_device_eval_batch_size 16 \\\n",
    "        --gradient_accumulation_steps 4 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"epoch\" \\\n",
    "        --save_total_limit 10 \\\n",
    "        --learning_rate {lr} \\\n",
    "        --weight_decay 0. \\\n",
    "        --warmup_ratio 0.03 \\\n",
    "        --logging_steps 1 \\\n",
    "        --deepspeed \"./train/configs/default_offload_opt_param.json\" \\\n",
    "        --output_dir  {model_location}/output_WarmupDecayLR_{lr} \\\n",
    "        --tf32 True > {log_location}/output_WarmupDecayLR_{lr}.txt\"\"\"\n",
    "\n",
    "    result = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436b36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
