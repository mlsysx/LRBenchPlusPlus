{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the evaluation package\n",
    "!git clone https://github.com/EleutherAI/lm-evaluation-harness ../evaluation/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_WarmupDecayLR_1218_2e-05',\n",
       " 'output_WarmupDecayLR_1218_3e-05',\n",
       " 'output_WarmupDecayLR_1218_4e-05',\n",
       " 'output_WarmupDecayLR_1218_5e-05']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tqdm \n",
    "model_PATH = ''\n",
    "files = os.listdir(model_PATH)\n",
    "txt_files = sorted([f for f in files if ('txt' not in f) & ('output' in f)])\n",
    "txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert the ds files -> model.bin \n",
    "model_PATH = ''\n",
    "\n",
    "os.chdir(model_PATH)\n",
    "model_files = os.listdir('./')\n",
    "\n",
    "for model_file in model_files:\n",
    "    print(model_file, '###' * 20)\n",
    "    for iter in [406, 813, 1218]:\n",
    "        print('checkpoint-',iter, '-------' * 10)\n",
    "        checkpoint_file = f'{model_file}/checkpoint-{iter}'\n",
    "        os.chdir(tmp_PATH + checkpoint_file)\n",
    "        files = os.listdir('./')\n",
    "        print(files)\n",
    "    \n",
    "        if 'pytorch_model.bin' not in files:\n",
    "            # !python zero_to_fp32.py . pytorch_model.bin\n",
    "            command = f'python zero_to_fp32.py . pytorch_model.bin'\n",
    "            subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4687/4687 [16:55<00:00,  4.61it/s]\n",
      " 10%|▉         | 4016/40168 [15:41<2:13:40,  4.51it/s]"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard \n",
    "log_PATH  = ''\n",
    "os.chdir(log_PATH)\n",
    "\n",
    "\n",
    "for model_path in txt_files:\n",
    "    command = f\"\"\"python ../evaluation/lm-evaluation-harness/main.py \\\n",
    "        --model hf-causal-experimental \\\n",
    "        --model_args pretrained=./{model_path},use_accelerate=True \\\n",
    "        --tasks arc_challenge \\\n",
    "        --device cuda:0 \\\n",
    "        --no_cache \\\n",
    "        --batch_size 8 \\\n",
    "        --num_fewshot 25 > ./{model_path}_arc.txt\"\"\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "    command = f\"\"\"python ../evaluation/lm-evaluation-harness/main.py \\\n",
    "        --model hf-causal-experimental \\\n",
    "        --model_args pretrained=./{model_path},use_accelerate=True \\\n",
    "        --tasks hellaswag \\\n",
    "        --device cuda:0 \\\n",
    "        --no_cache \\\n",
    "        --batch_size 8 \\\n",
    "        --num_fewshot 10 > ./{model_path}_hellaswag.txt\"\"\"\n",
    "    subprocess.run(command, shell=True)\n",
    "    \n",
    "    command = f\"\"\"python ../evaluation/lm-evaluation-harness/main.py \\\n",
    "        --model hf-causal-experimental \\\n",
    "        --model_args pretrained=./{model_path},use_accelerate=True \\\n",
    "        --tasks truthfulqa_mc \\\n",
    "        --device cuda:0 \\\n",
    "        --no_cache \\\n",
    "        --batch_size 8 \\\n",
    "        --num_fewshot 0 > ./{model_path}_truthfulqa.txt\"\"\"\n",
    "    subprocess.run(command, shell=True)  \n",
    "\n",
    "    command = f\"\"\"python ../evaluation/lm-evaluation-harness/main.py \\\n",
    "        --model hf-causal-experimental \\\n",
    "        --model_args pretrained=./{model_path},use_accelerate=True \\\n",
    "        --tasks hendrycksTest-abstract_algebra,hendrycksTest-anatomy,hendrycksTest-astronomy,hendrycksTest-business_ethics,hendrycksTest-clinical_knowledge,hendrycksTest-college_biology,hendrycksTest-college_chemistry,hendrycksTest-college_computer_science,hendrycksTest-college_mathematics,hendrycksTest-college_medicine,hendrycksTest-college_physics,hendrycksTest-computer_security,hendrycksTest-conceptual_physics,hendrycksTest-econometrics,hendrycksTest-electrical_engineering,hendrycksTest-elementary_mathematics,hendrycksTest-formal_logic,hendrycksTest-global_facts,hendrycksTest-high_school_biology,hendrycksTest-high_school_chemistry,hendrycksTest-high_school_computer_science,hendrycksTest-high_school_european_history,hendrycksTest-high_school_geography,hendrycksTest-high_school_government_and_politics,hendrycksTest-high_school_macroeconomics,hendrycksTest-high_school_mathematics,hendrycksTest-high_school_microeconomics,hendrycksTest-high_school_physics,hendrycksTest-high_school_psychology,hendrycksTest-high_school_statistics,hendrycksTest-high_school_us_history,hendrycksTest-high_school_world_history,hendrycksTest-human_aging,hendrycksTest-human_sexuality,hendrycksTest-international_law,hendrycksTest-jurisprudence,hendrycksTest-logical_fallacies,hendrycksTest-machine_learning,hendrycksTest-management,hendrycksTest-marketing,hendrycksTest-medical_genetics,hendrycksTest-miscellaneous,hendrycksTest-moral_disputes,hendrycksTest-moral_scenarios,hendrycksTest-nutrition,hendrycksTest-philosophy,hendrycksTest-prehistory,hendrycksTest-professional_accounting,hendrycksTest-professional_law,hendrycksTest-professional_medicine,hendrycksTest-professional_psychology,hendrycksTest-public_relations,hendrycksTest-security_studies,hendrycksTest-sociology,hendrycksTest-us_foreign_policy,hendrycksTest-virology,hendrycksTest-world_religions \\\n",
    "        --device cuda:0 \\\n",
    "        --no_cache \\\n",
    "        --batch_size 8 \\\n",
    "        --num_fewshot 2 > ./{model_path}_mmlu.txt\"\"\"\n",
    "    subprocess.run(command, shell=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.71it/s]\n",
      "100%|██████████| 4687/4687 [16:36<00:00,  4.70it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.94it/s]\n",
      "100%|██████████| 40168/40168 [2:02:14<00:00,  5.48it/s]  \n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.80it/s]\n",
      "100%|██████████| 5882/5882 [03:32<00:00, 27.70it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.77it/s]\n",
      "  3%|▎         | 1624/56060 [07:37<3:24:23,  4.44it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# untrained model\n",
    "PATH  = '/a/bear.cs.fiu.edu./disk/bear-a/users/hjin008/Desktop/lrbench4llm_w_results'\n",
    "os.chdir(PATH)\n",
    "\n",
    "command = f\"\"\"python ../evaluation/lm-evaluation-harness/main.py \\\n",
    "    --model hf-causal-experimental \\\n",
    "    --model_args pretrained=decapoda-research/llama-7b-hf,use_accelerate=True \\\n",
    "    --tasks arc_challenge \\\n",
    "    --device cuda:0 \\\n",
    "    --no_cache \\\n",
    "    --batch_size 8 \\\n",
    "    --num_fewshot 25 > ./untrained_model_arc.txt\"\"\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "command = f\"\"\"python ../evaluation/lm-evaluation-harness/main.py \\\n",
    "    --model hf-causal-experimental \\\n",
    "    --model_args pretrained=decapoda-research/llama-7b-hf,use_accelerate=True \\\n",
    "    --tasks hellaswag \\\n",
    "    --device cuda:0 \\\n",
    "    --no_cache \\\n",
    "    --batch_size 8 \\\n",
    "    --num_fewshot 10 > ./untrained_model_hellaswag.txt\"\"\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "command = f\"\"\"python ../evaluation/lm-evaluation-harness/main.py \\\n",
    "    --model hf-causal-experimental \\\n",
    "    --model_args pretrained=decapoda-research/llama-7b-hf,use_accelerate=True \\\n",
    "    --tasks truthfulqa_mc \\\n",
    "    --device cuda:0 \\\n",
    "    --no_cache \\\n",
    "    --batch_size 8 \\\n",
    "    --num_fewshot 0 > ./untrained_model_truthfulqa.txt\"\"\"\n",
    "subprocess.run(command, shell=True)  \n",
    "\n",
    "command = f\"\"\"python ../evaluation/lrbench4llm/lm-evaluation-harness/main.py \\\n",
    "    --model hf-causal-experimental \\\n",
    "    --model_args pretrained=decapoda-research/llama-7b-hf,use_accelerate=True \\\n",
    "    --tasks hendrycksTest-abstract_algebra,hendrycksTest-anatomy,hendrycksTest-astronomy,hendrycksTest-business_ethics,hendrycksTest-clinical_knowledge,hendrycksTest-college_biology,hendrycksTest-college_chemistry,hendrycksTest-college_computer_science,hendrycksTest-college_mathematics,hendrycksTest-college_medicine,hendrycksTest-college_physics,hendrycksTest-computer_security,hendrycksTest-conceptual_physics,hendrycksTest-econometrics,hendrycksTest-electrical_engineering,hendrycksTest-elementary_mathematics,hendrycksTest-formal_logic,hendrycksTest-global_facts,hendrycksTest-high_school_biology,hendrycksTest-high_school_chemistry,hendrycksTest-high_school_computer_science,hendrycksTest-high_school_european_history,hendrycksTest-high_school_geography,hendrycksTest-high_school_government_and_politics,hendrycksTest-high_school_macroeconomics,hendrycksTest-high_school_mathematics,hendrycksTest-high_school_microeconomics,hendrycksTest-high_school_physics,hendrycksTest-high_school_psychology,hendrycksTest-high_school_statistics,hendrycksTest-high_school_us_history,hendrycksTest-high_school_world_history,hendrycksTest-human_aging,hendrycksTest-human_sexuality,hendrycksTest-international_law,hendrycksTest-jurisprudence,hendrycksTest-logical_fallacies,hendrycksTest-machine_learning,hendrycksTest-management,hendrycksTest-marketing,hendrycksTest-medical_genetics,hendrycksTest-miscellaneous,hendrycksTest-moral_disputes,hendrycksTest-moral_scenarios,hendrycksTest-nutrition,hendrycksTest-philosophy,hendrycksTest-prehistory,hendrycksTest-professional_accounting,hendrycksTest-professional_law,hendrycksTest-professional_medicine,hendrycksTest-professional_psychology,hendrycksTest-public_relations,hendrycksTest-security_studies,hendrycksTest-sociology,hendrycksTest-us_foreign_policy,hendrycksTest-virology,hendrycksTest-world_religions \\\n",
    "    --device cuda:0 \\\n",
    "    --no_cache \\\n",
    "    --batch_size 8 \\\n",
    "    --num_fewshot 2 > ./untrained_model_mmlu.txt\"\"\"\n",
    "subprocess.run(command, shell=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda39_v2)",
   "language": "python",
   "name": "conda39_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
